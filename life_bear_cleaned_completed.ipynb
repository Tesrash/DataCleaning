{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiAxTF1tfh7H4+52nf1P8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tesrash/DataCleaning/blob/main/life_bear_cleaned_completed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjirF4c0TIcn",
        "outputId": "d050e1a7-17e1-4d59-ee5d-31bb2005d87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-018f6722487f>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head (first 10 rows):\n",
            "   id       login_id                 mail_address  \\\n",
            "0   1       sugimoto      sugimoto@lifebear.co.jp   \n",
            "1   2            kou     nakanishi@lifebear.co.jp   \n",
            "2   3         yusuke        yuozawa1208@gmail.com   \n",
            "3   4     entyan1106           endo1106@gmail.com   \n",
            "4   5         kuriki             kuriki@wavy4.com   \n",
            "5   6  mashiyamatest     mashiyama@lifebear.co.jp   \n",
            "6   7        deguchi  masayadeguchi1120@gmail.com   \n",
            "7   8         satomi      sammie.tommie@gmail.com   \n",
            "8   9      sugimoto2    sugimoto_1020@yahoo.co.jp   \n",
            "9  10       yumitaso           yumi1180@gmail.com   \n",
            "\n",
            "                           password           created_at          salt  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250  2012-01-13 22:54:05  yGwBKynnsctI   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa  2012-01-14 12:48:31  aha6EuRYCDvU   \n",
            "2  048261a8024ce51d379eb53cc51aaf33  2012-01-17 15:33:22  PVS59dPWk9BH   \n",
            "3  cd77a9dac26260a104facda5665eb3ab  2012-01-17 15:37:02  vLZI6TVCJowN   \n",
            "4  a026597c294cc48cd20ae361f10cbab1  2012-01-17 18:52:32  swFznWWk79fg   \n",
            "5  12e83b9e106735267e2addac7756065e  2012-01-17 22:18:54  gY9pUJiWJ51P   \n",
            "6  7549d3174922ff5751f4007c2cb689f9  2012-01-17 23:13:17  pyGHpE75ahpC   \n",
            "7  eeb4e8717d14055d922c1a0efd5e0d24  2012-01-23 22:17:21  GAgqqlJM8mPB   \n",
            "8  0495871de880e58decf74a18dda8392f  2012-02-21 12:06:45  0RiB4v0NMT8G   \n",
            "9  6989d9f78327393c86b9f4e0e0b23000  2012-02-21 13:36:08  XXc6C3XHCCdm   \n",
            "\n",
            "  birthday_on  gender  \n",
            "0  1984-11-09     0.0  \n",
            "1  1986-11-13     0.0  \n",
            "2  1984-12-08     0.0  \n",
            "3  1987-11-06     0.0  \n",
            "4  1986-10-21     0.0  \n",
            "5  1970-01-01     0.0  \n",
            "6         NaN     NaN  \n",
            "7  1970-01-01     1.0  \n",
            "8  1984-11-09     0.0  \n",
            "9  1988-08-11     1.0  \n",
            "\n",
            "Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3680442 entries, 0 to 3680441\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   id            int64  \n",
            " 1   login_id      object \n",
            " 2   mail_address  object \n",
            " 3   password      object \n",
            " 4   created_at    object \n",
            " 5   salt          object \n",
            " 6   birthday_on   object \n",
            " 7   gender        float64\n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 224.6+ MB\n",
            "None\n",
            "\n",
            "Description:\n",
            "                 id        gender\n",
            "count  3.680442e+06  2.362919e+06\n",
            "mean   4.918847e+06  6.488492e-01\n",
            "std    3.677878e+06  5.455685e-01\n",
            "min    1.000000e+00  0.000000e+00\n",
            "25%    1.339456e+06  0.000000e+00\n",
            "50%    4.577952e+06  1.000000e+00\n",
            "75%    8.188005e+06  1.000000e+00\n",
            "max    1.159593e+07  9.900000e+01\n"
          ]
        }
      ],
      "source": [
        "# prompt:  generate code to read csv \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\" print  head(first 10) , info, description and change the separator  to a ';', set to low memory = 'True'\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Print the head (first 10 rows)\n",
        "print(\"Head (first 10 rows):\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Print the info\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "# Print the description\n",
        "print(\"\\nDescription:\")\n",
        "print(df.describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code. Create a function to split /content/JapanLifeBar.csv into chunks with parameters for the file_path, chunk_size and output_directory. Include error checking. chunk_size should be by rows. export the chunks to csv\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunk_size, output_directory):\n",
        "  \"\"\"Splits a CSV file into smaller chunks.\n",
        "\n",
        "  Args:\n",
        "    file_path: Path to the input CSV file.\n",
        "    chunk_size: Number of rows per chunk.\n",
        "    output_directory: Directory to save the output chunks.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n",
        "      chunk.to_csv(os.path.join(output_directory, f\"chunk_{chunk_number}.csv\"), index=False)\n",
        "      print(f\"Chunk {chunk_number} saved successfully.\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # Number of rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv_into_chunks(file_path, chunk_size, output_directory)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjX1RE2zWZJx",
        "outputId": "1dabf72f-e266-40f5-fe8f-f83d8c942e21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 0 saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ddfc30b5c88a>:19: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved successfully.\n",
            "Chunk 2 saved successfully.\n",
            "Chunk 3 saved successfully.\n",
            "Chunk 4 saved successfully.\n",
            "Chunk 5 saved successfully.\n",
            "Chunk 6 saved successfully.\n",
            "Chunk 7 saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code to find any duplicates\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Print the head (first 10 rows)\n",
        "print(\"Head (first 10 rows):\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Print the info\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "# Print the description\n",
        "print(\"\\nDescription:\")\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunk_size, output_directory):\n",
        "  \"\"\"Splits a CSV file into smaller chunks.\n",
        "\n",
        "  Args:\n",
        "    file_path: Path to the input CSV file.\n",
        "    chunk_size: Number of rows per chunk.\n",
        "    output_directory: Directory to save the output chunks.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n",
        "      chunk.to_csv(os.path.join(output_directory, f\"chunk_{chunk_number}.csv\"), index=False)\n",
        "      print(f\"Chunk {chunk_number} saved successfully.\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # Number of rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv_into_chunks(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "  print(\"\\nDuplicate Rows:\")\n",
        "  print(duplicate_rows)\n",
        "else:\n",
        "  print(\"\\nNo duplicate rows found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk0vy_PDYL2w",
        "outputId": "acc915b7-ef71-4d4b-e3b5-67ddb43f0e0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-48f50decb4db>:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head (first 10 rows):\n",
            "   id       login_id                 mail_address  \\\n",
            "0   1       sugimoto      sugimoto@lifebear.co.jp   \n",
            "1   2            kou     nakanishi@lifebear.co.jp   \n",
            "2   3         yusuke        yuozawa1208@gmail.com   \n",
            "3   4     entyan1106           endo1106@gmail.com   \n",
            "4   5         kuriki             kuriki@wavy4.com   \n",
            "5   6  mashiyamatest     mashiyama@lifebear.co.jp   \n",
            "6   7        deguchi  masayadeguchi1120@gmail.com   \n",
            "7   8         satomi      sammie.tommie@gmail.com   \n",
            "8   9      sugimoto2    sugimoto_1020@yahoo.co.jp   \n",
            "9  10       yumitaso           yumi1180@gmail.com   \n",
            "\n",
            "                           password           created_at          salt  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250  2012-01-13 22:54:05  yGwBKynnsctI   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa  2012-01-14 12:48:31  aha6EuRYCDvU   \n",
            "2  048261a8024ce51d379eb53cc51aaf33  2012-01-17 15:33:22  PVS59dPWk9BH   \n",
            "3  cd77a9dac26260a104facda5665eb3ab  2012-01-17 15:37:02  vLZI6TVCJowN   \n",
            "4  a026597c294cc48cd20ae361f10cbab1  2012-01-17 18:52:32  swFznWWk79fg   \n",
            "5  12e83b9e106735267e2addac7756065e  2012-01-17 22:18:54  gY9pUJiWJ51P   \n",
            "6  7549d3174922ff5751f4007c2cb689f9  2012-01-17 23:13:17  pyGHpE75ahpC   \n",
            "7  eeb4e8717d14055d922c1a0efd5e0d24  2012-01-23 22:17:21  GAgqqlJM8mPB   \n",
            "8  0495871de880e58decf74a18dda8392f  2012-02-21 12:06:45  0RiB4v0NMT8G   \n",
            "9  6989d9f78327393c86b9f4e0e0b23000  2012-02-21 13:36:08  XXc6C3XHCCdm   \n",
            "\n",
            "  birthday_on  gender  \n",
            "0  1984-11-09     0.0  \n",
            "1  1986-11-13     0.0  \n",
            "2  1984-12-08     0.0  \n",
            "3  1987-11-06     0.0  \n",
            "4  1986-10-21     0.0  \n",
            "5  1970-01-01     0.0  \n",
            "6         NaN     NaN  \n",
            "7  1970-01-01     1.0  \n",
            "8  1984-11-09     0.0  \n",
            "9  1988-08-11     1.0  \n",
            "\n",
            "Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3680442 entries, 0 to 3680441\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   id            int64  \n",
            " 1   login_id      object \n",
            " 2   mail_address  object \n",
            " 3   password      object \n",
            " 4   created_at    object \n",
            " 5   salt          object \n",
            " 6   birthday_on   object \n",
            " 7   gender        float64\n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 224.6+ MB\n",
            "None\n",
            "\n",
            "Description:\n",
            "                 id        gender\n",
            "count  3.680442e+06  2.362919e+06\n",
            "mean   4.918847e+06  6.488492e-01\n",
            "std    3.677878e+06  5.455685e-01\n",
            "min    1.000000e+00  0.000000e+00\n",
            "25%    1.339456e+06  0.000000e+00\n",
            "50%    4.577952e+06  1.000000e+00\n",
            "75%    8.188005e+06  1.000000e+00\n",
            "max    1.159593e+07  9.900000e+01\n",
            "Chunk 0 saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-48f50decb4db>:38: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved successfully.\n",
            "Chunk 2 saved successfully.\n",
            "Chunk 3 saved successfully.\n",
            "Chunk 4 saved successfully.\n",
            "Chunk 5 saved successfully.\n",
            "Chunk 6 saved successfully.\n",
            "Chunk 7 saved successfully.\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "No duplicate rows found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code to find missing values\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Print the head (first 10 rows)\n",
        "print(\"Head (first 10 rows):\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Print the info\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "# Print the description\n",
        "print(\"\\nDescription:\")\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunk_size, output_directory):\n",
        "  \"\"\"Splits a CSV file into smaller chunks.\n",
        "\n",
        "  Args:\n",
        "    file_path: Path to the input CSV file.\n",
        "    chunk_size: Number of rows per chunk.\n",
        "    output_directory: Directory to save the output chunks.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n",
        "      chunk.to_csv(os.path.join(output_directory, f\"chunk_{chunk_number}.csv\"), index=False)\n",
        "      print(f\"Chunk {chunk_number} saved successfully.\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # Number of rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv_into_chunks(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Print the head (first 10 rows)\n",
        "print(\"Head (first 10 rows):\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Print the info\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "# Print the description\n",
        "print(\"\\nDescription:\")\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunk_size, output_directory):\n",
        "  \"\"\"Splits a CSV file into smaller chunks.\n",
        "\n",
        "  Args:\n",
        "    file_path: Path to the input CSV file.\n",
        "    chunk_size: Number of rows per chunk.\n",
        "    output_directory: Directory to save the output chunks.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n",
        "      chunk.to_csv(os.path.join(output_directory, f\"chunk_{chunk_number}.csv\"), index=False)\n",
        "      print(f\"Chunk {chunk_number} saved successfully.\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # Number of rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv_into_chunks(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "  print(\"\\nDuplicate Rows:\")\n",
        "  print(duplicate_rows)\n",
        "else:\n",
        "  print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "\n",
        "# Find missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FW6TNEJY9Rf",
        "outputId": "0ead0c48-fa41-4f0a-9499-ab2a6a029f60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-de3df579c12d>:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head (first 10 rows):\n",
            "   id       login_id                 mail_address  \\\n",
            "0   1       sugimoto      sugimoto@lifebear.co.jp   \n",
            "1   2            kou     nakanishi@lifebear.co.jp   \n",
            "2   3         yusuke        yuozawa1208@gmail.com   \n",
            "3   4     entyan1106           endo1106@gmail.com   \n",
            "4   5         kuriki             kuriki@wavy4.com   \n",
            "5   6  mashiyamatest     mashiyama@lifebear.co.jp   \n",
            "6   7        deguchi  masayadeguchi1120@gmail.com   \n",
            "7   8         satomi      sammie.tommie@gmail.com   \n",
            "8   9      sugimoto2    sugimoto_1020@yahoo.co.jp   \n",
            "9  10       yumitaso           yumi1180@gmail.com   \n",
            "\n",
            "                           password           created_at          salt  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250  2012-01-13 22:54:05  yGwBKynnsctI   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa  2012-01-14 12:48:31  aha6EuRYCDvU   \n",
            "2  048261a8024ce51d379eb53cc51aaf33  2012-01-17 15:33:22  PVS59dPWk9BH   \n",
            "3  cd77a9dac26260a104facda5665eb3ab  2012-01-17 15:37:02  vLZI6TVCJowN   \n",
            "4  a026597c294cc48cd20ae361f10cbab1  2012-01-17 18:52:32  swFznWWk79fg   \n",
            "5  12e83b9e106735267e2addac7756065e  2012-01-17 22:18:54  gY9pUJiWJ51P   \n",
            "6  7549d3174922ff5751f4007c2cb689f9  2012-01-17 23:13:17  pyGHpE75ahpC   \n",
            "7  eeb4e8717d14055d922c1a0efd5e0d24  2012-01-23 22:17:21  GAgqqlJM8mPB   \n",
            "8  0495871de880e58decf74a18dda8392f  2012-02-21 12:06:45  0RiB4v0NMT8G   \n",
            "9  6989d9f78327393c86b9f4e0e0b23000  2012-02-21 13:36:08  XXc6C3XHCCdm   \n",
            "\n",
            "  birthday_on  gender  \n",
            "0  1984-11-09     0.0  \n",
            "1  1986-11-13     0.0  \n",
            "2  1984-12-08     0.0  \n",
            "3  1987-11-06     0.0  \n",
            "4  1986-10-21     0.0  \n",
            "5  1970-01-01     0.0  \n",
            "6         NaN     NaN  \n",
            "7  1970-01-01     1.0  \n",
            "8  1984-11-09     0.0  \n",
            "9  1988-08-11     1.0  \n",
            "\n",
            "Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3680442 entries, 0 to 3680441\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   id            int64  \n",
            " 1   login_id      object \n",
            " 2   mail_address  object \n",
            " 3   password      object \n",
            " 4   created_at    object \n",
            " 5   salt          object \n",
            " 6   birthday_on   object \n",
            " 7   gender        float64\n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 224.6+ MB\n",
            "None\n",
            "\n",
            "Description:\n",
            "                 id        gender\n",
            "count  3.680442e+06  2.362919e+06\n",
            "mean   4.918847e+06  6.488492e-01\n",
            "std    3.677878e+06  5.455685e-01\n",
            "min    1.000000e+00  0.000000e+00\n",
            "25%    1.339456e+06  0.000000e+00\n",
            "50%    4.577952e+06  1.000000e+00\n",
            "75%    8.188005e+06  1.000000e+00\n",
            "max    1.159593e+07  9.900000e+01\n",
            "Chunk 0 saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-de3df579c12d>:38: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved successfully.\n",
            "Chunk 2 saved successfully.\n",
            "Chunk 3 saved successfully.\n",
            "Chunk 4 saved successfully.\n",
            "Chunk 5 saved successfully.\n",
            "Chunk 6 saved successfully.\n",
            "Chunk 7 saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-de3df579c12d>:61: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head (first 10 rows):\n",
            "   id       login_id                 mail_address  \\\n",
            "0   1       sugimoto      sugimoto@lifebear.co.jp   \n",
            "1   2            kou     nakanishi@lifebear.co.jp   \n",
            "2   3         yusuke        yuozawa1208@gmail.com   \n",
            "3   4     entyan1106           endo1106@gmail.com   \n",
            "4   5         kuriki             kuriki@wavy4.com   \n",
            "5   6  mashiyamatest     mashiyama@lifebear.co.jp   \n",
            "6   7        deguchi  masayadeguchi1120@gmail.com   \n",
            "7   8         satomi      sammie.tommie@gmail.com   \n",
            "8   9      sugimoto2    sugimoto_1020@yahoo.co.jp   \n",
            "9  10       yumitaso           yumi1180@gmail.com   \n",
            "\n",
            "                           password           created_at          salt  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250  2012-01-13 22:54:05  yGwBKynnsctI   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa  2012-01-14 12:48:31  aha6EuRYCDvU   \n",
            "2  048261a8024ce51d379eb53cc51aaf33  2012-01-17 15:33:22  PVS59dPWk9BH   \n",
            "3  cd77a9dac26260a104facda5665eb3ab  2012-01-17 15:37:02  vLZI6TVCJowN   \n",
            "4  a026597c294cc48cd20ae361f10cbab1  2012-01-17 18:52:32  swFznWWk79fg   \n",
            "5  12e83b9e106735267e2addac7756065e  2012-01-17 22:18:54  gY9pUJiWJ51P   \n",
            "6  7549d3174922ff5751f4007c2cb689f9  2012-01-17 23:13:17  pyGHpE75ahpC   \n",
            "7  eeb4e8717d14055d922c1a0efd5e0d24  2012-01-23 22:17:21  GAgqqlJM8mPB   \n",
            "8  0495871de880e58decf74a18dda8392f  2012-02-21 12:06:45  0RiB4v0NMT8G   \n",
            "9  6989d9f78327393c86b9f4e0e0b23000  2012-02-21 13:36:08  XXc6C3XHCCdm   \n",
            "\n",
            "  birthday_on  gender  \n",
            "0  1984-11-09     0.0  \n",
            "1  1986-11-13     0.0  \n",
            "2  1984-12-08     0.0  \n",
            "3  1987-11-06     0.0  \n",
            "4  1986-10-21     0.0  \n",
            "5  1970-01-01     0.0  \n",
            "6         NaN     NaN  \n",
            "7  1970-01-01     1.0  \n",
            "8  1984-11-09     0.0  \n",
            "9  1988-08-11     1.0  \n",
            "\n",
            "Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3680442 entries, 0 to 3680441\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   id            int64  \n",
            " 1   login_id      object \n",
            " 2   mail_address  object \n",
            " 3   password      object \n",
            " 4   created_at    object \n",
            " 5   salt          object \n",
            " 6   birthday_on   object \n",
            " 7   gender        float64\n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 224.6+ MB\n",
            "None\n",
            "\n",
            "Description:\n",
            "                 id        gender\n",
            "count  3.680442e+06  2.362919e+06\n",
            "mean   4.918847e+06  6.488492e-01\n",
            "std    3.677878e+06  5.455685e-01\n",
            "min    1.000000e+00  0.000000e+00\n",
            "25%    1.339456e+06  0.000000e+00\n",
            "50%    4.577952e+06  1.000000e+00\n",
            "75%    8.188005e+06  1.000000e+00\n",
            "max    1.159593e+07  9.900000e+01\n",
            "Chunk 0 saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-de3df579c12d>:91: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved successfully.\n",
            "Chunk 2 saved successfully.\n",
            "Chunk 3 saved successfully.\n",
            "Chunk 4 saved successfully.\n",
            "Chunk 5 saved successfully.\n",
            "Chunk 6 saved successfully.\n",
            "Chunk 7 saved successfully.\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "No duplicate rows found.\n",
            "\n",
            "Missing Values per Column:\n",
            "id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code, check for invalid emails. create a csv folder name \"junk\" and put any invalid emails found in it and save\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Function to validate email addresses using a regular expression\n",
        "def is_valid_email(email):\n",
        "  regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "  return re.match(regex, email) is not None\n",
        "\n",
        "\n",
        "# Create the \"junk\" directory if it doesn't exist\n",
        "junk_directory = \"/content/junk\"\n",
        "if not os.path.exists(junk_directory):\n",
        "  os.makedirs(junk_directory)\n",
        "\n",
        "# Find rows with invalid emails and save them to a CSV file\n",
        "invalid_email_rows = df[~df['email'].apply(is_valid_email)]\n",
        "if not invalid_email_rows.empty:\n",
        "    invalid_email_rows.to_csv(os.path.join(junk_directory, \"invalid_emails.csv\"), index=False)\n",
        "    print(\"Rows with invalid emails saved to /content/junk/invalid_emails.csv\")\n",
        "else:\n",
        "    print(\"No rows with invalid emails found.\")\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "  print(\"\\nDuplicate Rows:\")\n",
        "  print(duplicate_rows)\n",
        "else:\n",
        "  print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "\n",
        "# Find missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "KyPT9VikaXPc",
        "outputId": "4efe7201-f380-405f-8e02-0748ae800ca6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-3317075c3094>:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'email'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'email'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3317075c3094>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Find rows with invalid emails and save them to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0minvalid_email_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'email'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_valid_email\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minvalid_email_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0minvalid_email_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjunk_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"invalid_emails.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'email'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Function to validate email addresses using a regular expression\n",
        "def is_valid_email(email):\n",
        "  regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "  return re.match(regex, email) is not None\n",
        "\n",
        "\n",
        "# Create the \"junk\" directory if it doesn't exist\n",
        "junk_directory = \"/content/junk\"\n",
        "if not os.path.exists(junk_directory):\n",
        "  os.makedirs(junk_directory)\n",
        "\n",
        "# **Get the correct email column name (assuming it's 'login_id' based on global variables)**\n",
        "email_column_name = 'login_id'  # Replace 'login_id' with the actual column name if different\n",
        "\n",
        "# Find rows with invalid emails and save them to a CSV file\n",
        "invalid_email_rows = df[~df[email_column_name].apply(is_valid_email)] # **Use the correct column name here**\n",
        "if not invalid_email_rows.empty:\n",
        "    invalid_email_rows.to_csv(os.path.join(junk_directory, \"invalid_emails.csv\"), index=False)\n",
        "    print(\"Rows with invalid emails saved to /content/junk/invalid_emails.csv\")\n",
        "else:\n",
        "    print(\"No rows with invalid emails found.\")\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "  print(\"\\nDuplicate Rows:\")\n",
        "  print(duplicate_rows)\n",
        "else:\n",
        "  print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "\n",
        "# Find missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "_Sd9p60bcD-s",
        "outputId": "0c970765-0a15-4b15-8d16-8c1e1eaecf01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-bd984f83a30e>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bd984f83a30e>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Find rows with invalid emails and save them to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0minvalid_email_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memail_column_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_valid_email\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# **Use the correct column name here**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minvalid_email_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0minvalid_email_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjunk_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"invalid_emails.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-bd984f83a30e>\u001b[0m in \u001b[0;36mis_valid_email\u001b[0;34m(email)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_valid_email\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"Try to apply the pattern at the start of the string, returning\n\u001b[1;32m    189\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Function to validate email addresses using a regular expression\n",
        "def is_valid_email(email):\n",
        "    # Check if email is a string before applying regex\n",
        "    if isinstance(email, str):\n",
        "        regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "        return re.match(regex, email) is not None\n",
        "    else:\n",
        "        return False  # Treat non-string values as invalid emails\n",
        "\n",
        "\n",
        "# Create the \"junk\" directory if it doesn't exist\n",
        "junk_directory = \"/content/junk\"\n",
        "if not os.path.exists(junk_directory):\n",
        "    os.makedirs(junk_directory)\n",
        "\n",
        "# Get the correct email column name (assuming it's 'login_id' based on global variables)\n",
        "email_column_name = 'login_id'  # Replace 'login_id' with the actual column name if different\n",
        "\n",
        "# Find rows with invalid emails and save them to a CSV file\n",
        "invalid_email_rows = df[~df[email_column_name].apply(is_valid_email)]  # Use the correct column name here\n",
        "if not invalid_email_rows.empty:\n",
        "    invalid_email_rows.to_csv(os.path.join(junk_directory, \"invalid_emails.csv\"), index=False)\n",
        "    print(\"Rows with invalid emails saved to /content/junk/invalid_emails.csv\")\n",
        "else:\n",
        "    print(\"No rows with invalid emails found.\")\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "    print(\"\\nDuplicate Rows:\")\n",
        "    print(duplicate_rows)\n",
        "else:\n",
        "    print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "\n",
        "# Find missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJxzLzqOcYsr",
        "outputId": "bfc8de75-fc9b-4294-8f8b-3ade3a0678e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b724a973c92e>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with invalid emails saved to /content/junk/invalid_emails.csv\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "No duplicate rows found.\n",
            "\n",
            "Missing Values per Column:\n",
            "id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code to check for invalid characters\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Function to validate email addresses using a regular expression\n",
        "def is_valid_email(email):\n",
        "    # Check if email is a string before applying regex\n",
        "    if isinstance(email, str):\n",
        "        regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "        return re.match(regex, email) is not None\n",
        "    else:\n",
        "        return False  # Treat non-string values as invalid emails\n",
        "\n",
        "\n",
        "# Create the \"junk\" directory if it doesn't exist\n",
        "junk_directory = \"/content/junk\"\n",
        "if not os.path.exists(junk_directory):\n",
        "    os.makedirs(junk_directory)\n",
        "\n",
        "# Get the correct email column name (assuming it's 'login_id' based on global variables)\n",
        "email_column_name = 'login_id'  # Replace 'login_id' with the actual column name if different\n",
        "\n",
        "# Find rows with invalid emails and save them to a CSV file\n",
        "invalid_email_rows = df[~df[email_column_name].apply(is_valid_email)]  # Use the correct column name here\n",
        "if not invalid_email_rows.empty:\n",
        "    invalid_email_rows.to_csv(os.path.join(junk_directory, \"invalid_emails.csv\"), index=False)\n",
        "    print(\"Rows with invalid emails saved to /content/junk/invalid_emails.csv\")\n",
        "else:\n",
        "    print(\"No rows with invalid emails found.\")\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "    print(\"\\nDuplicate Rows:\")\n",
        "    print(duplicate_rows)\n",
        "else:\n",
        "    print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "\n",
        "# Find missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(missing_values)\n",
        "\n",
        "\n",
        "def contains_invalid_chars(text, allowed_chars=None):\n",
        "  \"\"\"Checks if a text string contains invalid characters.\n",
        "\n",
        "  Args:\n",
        "    text: The text string to check.\n",
        "    allowed_chars: A string containing the allowed characters (optional).\n",
        "                   If None, only alphanumeric characters and spaces are allowed.\n",
        "\n",
        "  Returns:\n",
        "    True if the text contains invalid characters, False otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  if allowed_chars is None:\n",
        "    allowed_chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \"\n",
        "  return any(char not in allowed_chars for char in text)\n",
        "\n",
        "# Example usage\n",
        "# Find rows with invalid characters in the 'name' column\n",
        "\n",
        "# Assuming you want to restrict names to alphanumeric characters and spaces:\n",
        "# allowed_characters_for_name = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 '\n",
        "\n",
        "# Find rows with invalid characters in the 'name' column\n",
        "# invalid_name_rows = df[df['name'].apply(lambda name: contains_invalid_chars(name, allowed_characters_for_name))]\n",
        "# if not invalid_name_rows.empty:\n",
        "#     invalid_name_rows.to_csv(os.path.join(junk_directory, \"invalid_names.csv\"), index=False)\n",
        "#     print(\"Rows with invalid names saved to /content/junk/invalid_names.csv\")\n",
        "# else:\n",
        "#     print(\"No rows with invalid names found.\")\n",
        "\n",
        "\n",
        "# You can repeat this check for other columns as needed, adjusting the allowed characters\n",
        "\n",
        "# Iterate over columns and check for invalid characters\n",
        "for column in df.columns:\n",
        "    invalid_rows = df[df[column].apply(lambda x: isinstance(x, str) and contains_invalid_chars(x))]\n",
        "    if not invalid_rows.empty:\n",
        "        invalid_rows.to_csv(os.path.join(junk_directory, f\"invalid_{column}.csv\"), index=False)\n",
        "        print(f\"Rows with invalid characters in '{column}' saved to /content/junk/invalid_{column}.csv\")\n",
        "    else:\n",
        "        print(f\"No rows with invalid characters found in '{column}'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkagQNODc09q",
        "outputId": "db666523-4a93-410b-d2f0-939028049cb0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3b301c47442b>:9: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with invalid emails saved to /content/junk/invalid_emails.csv\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "No duplicate rows found.\n",
            "\n",
            "Missing Values per Column:\n",
            "id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "No rows with invalid characters found in 'id'.\n",
            "Rows with invalid characters in 'login_id' saved to /content/junk/invalid_login_id.csv\n",
            "Rows with invalid characters in 'mail_address' saved to /content/junk/invalid_mail_address.csv\n",
            "No rows with invalid characters found in 'password'.\n",
            "Rows with invalid characters in 'created_at' saved to /content/junk/invalid_created_at.csv\n",
            "No rows with invalid characters found in 'salt'.\n",
            "Rows with invalid characters in 'birthday_on' saved to /content/junk/invalid_birthday_on.csv\n",
            "No rows with invalid characters found in 'gender'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert the first letter in each column head from lower case to upper case. update this code\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Rename columns to convert the first letter of each column header to uppercase\n",
        "new_columns = {col: col[0].upper() + col[1:] for col in df.columns}\n",
        "df = df.rename(columns=new_columns)\n",
        "\n",
        "# Print the head (first 10 rows)\n",
        "print(\"Head (first 10 rows):\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Print the info\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "# Print the description\n",
        "print(\"\\nDescription:\")\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunk_size, output_directory):\n",
        "  \"\"\"Splits a CSV file into smaller chunks.\n",
        "\n",
        "  Args:\n",
        "    file_path: Path to the input CSV file.\n",
        "    chunk_size: Number of rows per chunk.\n",
        "    output_directory: Directory to save the output chunks.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n",
        "      chunk.to_csv(os.path.join(output_directory, f\"chunk_{chunk_number}.csv\"), index=False)\n",
        "      print(f\"Chunk {chunk_number} saved successfully.\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\"\n",
        "chunk_size = 500000  # Number of rows per chunk\n",
        "output_directory = \"/content/chunks\"\n",
        "\n",
        "split_csv_into_chunks(file_path, chunk_size, output_directory)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "  print(\"\\nDuplicate Rows:\")\n",
        "  print(duplicate_rows)\n",
        "else:\n",
        "  print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "\n",
        "# Find missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(missing_values)\n",
        "\n",
        "\n",
        "# Read the CSV file with specified parameters\n",
        "df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n",
        "\n",
        "# Function to validate email addresses using a regular expression\n",
        "def is_valid_email(email):\n",
        "    # Check if email is a string before applying regex\n",
        "    if isinstance(email, str):\n",
        "        regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "        return re.match(regex, email) is not None\n",
        "    else:\n",
        "        return False  # Treat non-string values as invalid emails\n",
        "\n",
        "\n",
        "# Create the \"junk\" directory if it doesn't exist\n",
        "junk_directory = \"/content/junk\"\n",
        "if not os.path.exists(junk_directory):\n",
        "    os.makedirs(junk_directory)\n",
        "\n",
        "# Get the correct email column name (assuming it's 'login_id' based on global variables)\n",
        "email_column_name = 'login_id'  # Replace 'login_id' with the actual column name if different\n",
        "\n",
        "# Find rows with invalid emails and save them to a CSV file\n",
        "invalid_email_rows = df[~df[email_column_name].apply(is_valid_email)]  # Use the correct column name here\n",
        "if not invalid_email_rows.empty:\n",
        "    invalid_email_rows.to_csv(os.path.join(junk_directory, \"invalid_emails.csv\"), index=False)\n",
        "    print(\"Rows with invalid emails saved to /content/junk/invalid_emails.csv\")\n",
        "else:\n",
        "    print(\"No rows with invalid emails found.\")\n",
        "\n",
        "\n",
        "# Find duplicate rows based on all columns\n",
        "duplicate_rows = df[df.duplicated(keep=False)]\n",
        "\n",
        "# Print the number of duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "# Print the duplicate rows\n",
        "if not duplicate_rows.empty:\n",
        "    print(\"\\nDuplicate Rows:\")\n",
        "    print(duplicate_rows)\n",
        "else:\n",
        "    print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "\n",
        "# Find missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Print the number of missing values for each column\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(missing_values)\n",
        "\n",
        "\n",
        "\n",
        "def contains_invalid_chars(text, allowed_chars=None):\n",
        "  \"\"\"Checks if a text string contains invalid characters.\n",
        "\n",
        "  Args:\n",
        "    text: The text string to check.\n",
        "    allowed_chars: A string containing the allowed characters (optional).\n",
        "                   If None, only alphanumeric characters and spaces are allowed.\n",
        "\n",
        "  Returns:\n",
        "    True if the text contains invalid characters, False otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  if allowed_chars is None:\n",
        "    allowed_chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \"\n",
        "  return any(char not in allowed_chars for char in text)\n",
        "\n",
        "# Example usage\n",
        "# Find rows with invalid characters in the 'name' column\n",
        "\n",
        "# Assuming you want to restrict names to alphanumeric characters and spaces:\n",
        "# allowed_characters_for_name = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 '\n",
        "\n",
        "# Find rows with invalid characters in the 'name' column\n",
        "# invalid_name_rows = df[df['name'].apply(lambda name: contains_invalid_chars(name, allowed_characters_for_name))]\n",
        "# if not invalid_name_rows.empty:\n",
        "#     invalid_name_rows.to_csv(os.path.join(junk_directory, \"invalid_names.csv\"), index=False)\n",
        "#     print(\"Rows with invalid names saved to /content/junk/invalid_names.csv\")\n",
        "# else:\n",
        "#     print(\"No rows with invalid names found.\")\n",
        "\n",
        "\n",
        "# You can repeat this check for other columns as needed, adjusting the allowed characters\n",
        "\n",
        "# Iterate over columns and check for invalid characters\n",
        "for column in df.columns:\n",
        "    invalid_rows = df[df[column].apply(lambda x: isinstance(x, str) and contains_invalid_chars(x))]\n",
        "    if not invalid_rows.empty:\n",
        "        invalid_rows.to_csv(os.path.join(junk_directory, f\"invalid_{column}.csv\"), index=False)\n",
        "        print(f\"Rows with invalid characters in '{column}' saved to /content/junk/invalid_{column}.csv\")\n",
        "    else:\n",
        "        print(f\"No rows with invalid characters found in '{column}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9jU02ZkdMZS",
        "outputId": "7e848a51-7baa-4fa4-ebb1-ef6e20c7360e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-fe07f9a59f56>:9: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head (first 10 rows):\n",
            "   Id       Login_id                 Mail_address  \\\n",
            "0   1       sugimoto      sugimoto@lifebear.co.jp   \n",
            "1   2            kou     nakanishi@lifebear.co.jp   \n",
            "2   3         yusuke        yuozawa1208@gmail.com   \n",
            "3   4     entyan1106           endo1106@gmail.com   \n",
            "4   5         kuriki             kuriki@wavy4.com   \n",
            "5   6  mashiyamatest     mashiyama@lifebear.co.jp   \n",
            "6   7        deguchi  masayadeguchi1120@gmail.com   \n",
            "7   8         satomi      sammie.tommie@gmail.com   \n",
            "8   9      sugimoto2    sugimoto_1020@yahoo.co.jp   \n",
            "9  10       yumitaso           yumi1180@gmail.com   \n",
            "\n",
            "                           Password           Created_at          Salt  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250  2012-01-13 22:54:05  yGwBKynnsctI   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa  2012-01-14 12:48:31  aha6EuRYCDvU   \n",
            "2  048261a8024ce51d379eb53cc51aaf33  2012-01-17 15:33:22  PVS59dPWk9BH   \n",
            "3  cd77a9dac26260a104facda5665eb3ab  2012-01-17 15:37:02  vLZI6TVCJowN   \n",
            "4  a026597c294cc48cd20ae361f10cbab1  2012-01-17 18:52:32  swFznWWk79fg   \n",
            "5  12e83b9e106735267e2addac7756065e  2012-01-17 22:18:54  gY9pUJiWJ51P   \n",
            "6  7549d3174922ff5751f4007c2cb689f9  2012-01-17 23:13:17  pyGHpE75ahpC   \n",
            "7  eeb4e8717d14055d922c1a0efd5e0d24  2012-01-23 22:17:21  GAgqqlJM8mPB   \n",
            "8  0495871de880e58decf74a18dda8392f  2012-02-21 12:06:45  0RiB4v0NMT8G   \n",
            "9  6989d9f78327393c86b9f4e0e0b23000  2012-02-21 13:36:08  XXc6C3XHCCdm   \n",
            "\n",
            "  Birthday_on  Gender  \n",
            "0  1984-11-09     0.0  \n",
            "1  1986-11-13     0.0  \n",
            "2  1984-12-08     0.0  \n",
            "3  1987-11-06     0.0  \n",
            "4  1986-10-21     0.0  \n",
            "5  1970-01-01     0.0  \n",
            "6         NaN     NaN  \n",
            "7  1970-01-01     1.0  \n",
            "8  1984-11-09     0.0  \n",
            "9  1988-08-11     1.0  \n",
            "\n",
            "Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3680442 entries, 0 to 3680441\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   Id            int64  \n",
            " 1   Login_id      object \n",
            " 2   Mail_address  object \n",
            " 3   Password      object \n",
            " 4   Created_at    object \n",
            " 5   Salt          object \n",
            " 6   Birthday_on   object \n",
            " 7   Gender        float64\n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 224.6+ MB\n",
            "None\n",
            "\n",
            "Description:\n",
            "                 Id        Gender\n",
            "count  3.680442e+06  2.362919e+06\n",
            "mean   4.918847e+06  6.488492e-01\n",
            "std    3.677878e+06  5.455685e-01\n",
            "min    1.000000e+00  0.000000e+00\n",
            "25%    1.339456e+06  0.000000e+00\n",
            "50%    4.577952e+06  1.000000e+00\n",
            "75%    8.188005e+06  1.000000e+00\n",
            "max    1.159593e+07  9.900000e+01\n",
            "Chunk 0 saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-fe07f9a59f56>:43: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for chunk_number, chunk in enumerate(pd.read_csv(file_path, sep=';', chunksize=chunk_size, low_memory=True)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1 saved successfully.\n",
            "Chunk 2 saved successfully.\n",
            "Chunk 3 saved successfully.\n",
            "Chunk 4 saved successfully.\n",
            "Chunk 5 saved successfully.\n",
            "Chunk 6 saved successfully.\n",
            "Chunk 7 saved successfully.\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "No duplicate rows found.\n",
            "\n",
            "Missing Values per Column:\n",
            "Id                    0\n",
            "Login_id              2\n",
            "Mail_address          0\n",
            "Password              0\n",
            "Created_at            0\n",
            "Salt                  0\n",
            "Birthday_on     3294901\n",
            "Gender          1317523\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-fe07f9a59f56>:88: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv\", sep=';', low_memory=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with invalid emails saved to /content/junk/invalid_emails.csv\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "No duplicate rows found.\n",
            "\n",
            "Missing Values per Column:\n",
            "id                    0\n",
            "login_id              2\n",
            "mail_address          0\n",
            "password              0\n",
            "created_at            0\n",
            "salt                  0\n",
            "birthday_on     3294901\n",
            "gender          1317523\n",
            "dtype: int64\n",
            "No rows with invalid characters found in 'id'.\n",
            "Rows with invalid characters in 'login_id' saved to /content/junk/invalid_login_id.csv\n",
            "Rows with invalid characters in 'mail_address' saved to /content/junk/invalid_mail_address.csv\n",
            "No rows with invalid characters found in 'password'.\n",
            "Rows with invalid characters in 'created_at' saved to /content/junk/invalid_created_at.csv\n",
            "No rows with invalid characters found in 'salt'.\n",
            "Rows with invalid characters in 'birthday_on' saved to /content/junk/invalid_birthday_on.csv\n",
            "No rows with invalid characters found in 'gender'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: update this code,  merge all 8 chunk back together and save as a csv name \"merge\"\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory where the chunks are stored\n",
        "chunk_directory = \"/content/chunks\"\n",
        "\n",
        "# Create an empty list to store the DataFrames of each chunk\n",
        "chunk_dfs = []\n",
        "\n",
        "# Iterate through the files in the chunk directory\n",
        "for filename in os.listdir(chunk_directory):\n",
        "  if filename.startswith(\"chunk_\") and filename.endswith(\".csv\"):\n",
        "    filepath = os.path.join(chunk_directory, filename)\n",
        "    # Read each chunk into a DataFrame and append it to the list\n",
        "    chunk_dfs.append(pd.read_csv(filepath))\n",
        "\n",
        "# Concatenate all the DataFrames in the list into a single DataFrame\n",
        "merged_df = pd.concat(chunk_dfs, ignore_index=True)\n",
        "\n",
        "# Save the merged DataFrame to a CSV file named \"merge.csv\"\n",
        "merged_df.to_csv(\"/content/merge.csv\", index=False)\n",
        "\n",
        "print(\"Merged CSV file saved as /content/merge.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKamsxrAg3j8",
        "outputId": "02cc1daf-3d19-4e5e-c85d-157dcce42e3d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-429acad3953d>:17: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  chunk_dfs.append(pd.read_csv(filepath))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged CSV file saved as /content/merge.csv\n"
          ]
        }
      ]
    }
  ]
}